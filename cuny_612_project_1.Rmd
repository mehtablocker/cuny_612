---
title: "CUNY 612"
subtitle: "Project 1"
author: "mehtablocker"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output:
  html_document:
    toc: true
    toc_float: true
editor_options: 
  chunk_output_type: console
---

<style type="text/css">
h3 {
  color: DarkBlue;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```

<br>

###Intro

__For this project we will build a simple recommender system using a toy dataset of movie reviews.__

<br>

###SQL Preparation

Make sure you have MySQL installed. Then please run the following sql script in order to create the movie_review database: 
[Github_Link](https://raw.githubusercontent.com/mehtablocker/cuny_607/master/cuny_607_week_2_hw.sql)


###Load libraries

```{r load_libraries, message=FALSE, warning=FALSE}
library(dplyr)
library(RMySQL)
library(dbplyr)
library(tidytext)
library(knitr)
```


###Loading the SQL tables into R

Be sure you use the correct username and password for your particular MySQL setup!

```{r load_tables}
mr_db <- src_mysql(dbname = 'movie_reviews', username = 'user', password = 'password')

movie_df <- tbl(mr_db, "movie") %>% collect(n=Inf)
critic_df <- tbl(mr_db, "critic") %>% collect(n=Inf)
review_df <- tbl(mr_db, "review") %>% collect(n=Inf)

review_enhanced_df <- review_df %>% 
  left_join(movie_df, by="movie_id") %>% 
  left_join(critic_df, by="critic_id") %>% 
  arrange(title, desc(movie_rating))

review_enhanced_df %>% head() %>% kable()

```

<br>

###Create matrix

We start by using the tidy dataframe to create a user-item matrix.

```{r create_mat}
ui_mat <- review_enhanced_df %>% 
  arrange(critic_name, title) %>% 
  cast_sparse(critic_name, title, movie_rating) %>% 
  as.matrix()
ui_mat %>% kable()
```

###Make sparse

We randomly create missing values in the matrix.

```{r make_sparse}
pct_na <- 0.2
n_na <- round( pct_na * length(ui_mat), 0)
na_ind <- sample(1:length(ui_mat), n_na, replace = F)
ui_mat[na_ind] <- NA
ui_mat %>% kable()
```

<br>

###Split the data

Next we break the matrix into training and test values.

```{r split_data}
pct_test <- 0.2
n_test <- round( pct_test * sum(!is.na(ui_mat)), 0)
test_ind <- sample((1:length(ui_mat))[-na_ind], n_test, replace = F)
train_ind <- (1:length(ui_mat))[-c(na_ind, test_ind)]

train_mat <- ui_mat
train_mat[test_ind] <- NA
test_mat <- ui_mat
test_mat[train_ind] <- NA

train_mat %>% kable()
test_mat %>% kable()
```

<br>

###Raw average model

Using the training data, we compute the overall average and make predictions for user-item combinations with that one value. We calculate the RMSE of our predictions on both the training and test data.

```{r raw_avg}
predictions_model_1 <- mean(train_mat, na.rm=T)

rmse <- function(predicted, observed, rmna=FALSE){
  sqrt(mean((predicted - observed)^2, na.rm=rmna))
}

### Train RMSE
train_rmse_model_1 <- rmse(predictions_model_1, train_mat[!is.na(train_mat)])
train_rmse_model_1

### Test RMSE
test_rmse_model_1 <- rmse(predictions_model_1, test_mat[!is.na(test_mat)])
test_rmse_model_1
```

<br>

###Baseline predictor model

We try to improve upon the raw average model by calculating a "bias" for each user and each item (movie, in this case) using the training data. We will do this by taking averages for each row and column, and subtracting the overall mean. Effectively we are estimating how much each user (ie, row) and each item (ie, movie, or column) is above or below the overall average.  

Then to make predictions for each user-item combination we will take the raw average and add the according user and item bias. We calculate the RMSE of our predictions on both the training and test data.

```{r baseline_pred}
row_bias <- rowMeans(train_mat, na.rm=T) - mean(train_mat, na.rm=T)
col_bias <- colMeans(train_mat, na.rm=T) - mean(train_mat, na.rm=T)

predictions_df_model_2 <- expand.grid(row_bias=row_bias, col_bias=col_bias) %>% 
  cbind(expand.grid(user=names(row_bias), item=names(col_bias))) %>% 
  mutate(raw_avg = mean(train_mat, na.rm=T), prediction = raw_avg + row_bias + col_bias)

predictions_df_model_2 %>% head() %>% kable()

predictions_mat_model_2 <- predictions_df_model_2 %>% 
  arrange(user, item) %>% 
  cast_sparse(user, item, prediction) %>% 
  as.matrix()

predictions_mat_model_2 %>% kable()

### Train RMSE
train_rmse_model_2 <- rmse(predictions_mat_model_2, train_mat, rmna=T)
train_rmse_model_2

### Test RMSE
test_rmse_model_2 <- rmse(predictions_mat_model_2, test_mat, rmna=T)
test_rmse_model_2
```

<br>

###Model comparison summary

We see that for the slightly more complicated second model we get a Test RMSE of __`r round(test_rmse_model_2, 2)`__ compared to __`r round(test_rmse_model_1, 2)`__ for the simple first model, which is a __`r ifelse(test_rmse_model_2 < test_rmse_model_1, "better", "worse")`__ result.

We would expect the second model to be more accurate, given it contains more pertinent information. But also keep in mind natural variance can play a larger role in a toy dataset with small sample sizes. The next step would be to repeat this analysis on a larger dataset.
