---
title: "CUNY 612"
subtitle: "Final Project"
author: "mehtablocker"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output:
  html_document:
    toc: true
    toc_float: true
editor_options: 
  chunk_output_type: console
---

<style type="text/css">
h3 {
  color: DarkBlue;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```

<br>

###Intro

__We will build collaborative filtering-based recommender systems for movie ratings using a regression-based technique to account for user and item biases, as well as matrix factorization methods to find latent themes.__

<br>

###Load libraries

```{r load_libraries, message=FALSE, warning=FALSE}
library(dplyr)
library(ggplot2)
library(recommenderlab)
library(knitr)
```

<br>

###Get data

The MovieLens dataset of 100,000 movie ratings is provided in User-Item matrix form in the `recommenderlab` library. Each row represents a user, and NAs are listed for any movie not rated.

```{r get_data}
data(MovieLense)
ui_mat <- as(MovieLense, "matrix")
ui_mat[1:5, 1:5] %>% kable()
```

<br>

###Split data

We start by breaking our data into training and test sets.

```{r split_data}

### Create test and train indexes
pct_test <- 0.2
n_test <- round( pct_test * sum(!is.na(ui_mat)), 0)
na_ind <- which(is.na(ui_mat))
test_ind <- sample((1:length(ui_mat))[-na_ind], n_test, replace = F)
train_ind <- (1:length(ui_mat))[-c(na_ind, test_ind)]

### Break matrix into test and train
train_mat <- ui_mat
train_mat[test_ind] <- NA
test_mat <- ui_mat
test_mat[train_ind] <- NA

train_mat[1:5, 1:5] %>% kable()
test_mat[1:5, 1:5] %>% kable()
```

<br>

###Normalize data

Subtracting a “baseline” from user-item ratings is a common first step in the building of recommender systems. This is usually done by calculating and then subtracting a global average (i.e., “global bias”) plus an average for each user (i.e., “user bias”) and an average for each item (i.e., “item bias”) from each rating.  

```{r norm_data}

### Calculate baseline and normalize matrix
global_mean <- mean(train_mat, na.rm = T)
u_bias_vec <- unname(rowMeans(train_mat, na.rm=T)) - global_mean
u_bias_mat <- matrix(u_bias_vec, nrow=nrow(train_mat), ncol=ncol(train_mat))
i_bias_vec <- unname(colMeans(train_mat, na.rm=T)) - global_mean
i_bias_mat <- matrix(i_bias_vec, nrow=nrow(train_mat), ncol=ncol(train_mat), byrow=T)
baseline <- global_mean + u_bias_mat + i_bias_mat
train_mat_norm_simp <- train_mat - baseline
train_mat_norm_simp[is.na(train_mat_norm_simp)] <- 0

```

<br>

###Alternate baseline

Using averages for the baseline calculation seems overly simplistic. It might be fine if the user-item matrix were more densely populated. But with such a sparse matrix, how do we know that a user’s average is not the result of consuming a few particularly good (or bad) items?  

A seemingly better procedure for obtaining biases would be to run a regression where all users and items are the predictor variables and the ratings are the response variable. Then the coefficients for each user and item would be the “user bias” and “item bias”, and the intercept would be the “global bias”.

####Implementation

One way of achieving such a regression would be to do a form of “one-hot encoding” for every user, item, and rating. For example, let's say Joe gave Toy Story a 5 rating. Rather than having a user-item matrix with a 5 in the cell that corresponds to Joe and Toy Story, we instead have a matrix where the columns are every user and every movie, and an additional column is the rating. In that case our first row would have a zero in every cell except a 1 for Joe, a 1 for Toy Story, and a 5 for Rating. And the same idea for the next row (rating.)  

We could then run a regression where the Rating column is the response variable and all of the other columns are the predictors. The result of the regression would give a coefficient for every user and item, as well as an intercept.  

For example:  

```{r impl_ex}
A_b <- tibble(Bob=c(0, 0), Joe=c(1,0), Nicole=c(0, 1), `all other users...`=c("0...", "0..."), `Toy Story`=c(1,0), `Forrest Gump`=c(0,1), `all other movies...`=c("0...", "0..."), Rating=c(5, 4))

A_b %>% kable()
```

####Considerations

Computing power is a concern when running a regression of this size. Even with the moderately sized MovieLens 100k ratings, we will be computing the coefficients for over 25,000 predictor variables. This is computationally expensive.  

To mitigate this expense we can use a linear algebra trick. Instead of running a regression with our one-hot encode matrix A and our ratings vector b to find our coefficient vector x, we can use the matrix A^T^ A and the vector A^T^ b, and then directly solve for x. This is a much cheaper computational task. To make this work we will need to employ a couple other tricks (to account for the intercept as well as the fact that the A^T^ A matrix is singular), but overall it is a fairly easy procedure.

```{r alt_baseline}

### Create binary matrix out of appropriate quadrants
ur_mat <- train_mat
ur_mat[!is.na(ur_mat)] <- 1
ur_mat[is.na(ur_mat)] <- 0
ul_mat <- matrix(0, nrow=nrow(train_mat), ncol=nrow(train_mat))
diag(ul_mat) <- rowSums(ur_mat, na.rm=T)
lr_mat <- matrix(0, nrow=ncol(train_mat), ncol=ncol(train_mat))
diag(lr_mat) <- colSums(ur_mat, na.rm=T)
ll_mat <- t(ur_mat)
train_mat_bin <- rbind(cbind(ul_mat, ur_mat), cbind(ll_mat, lr_mat))

### Create means vectors
u_means_vec <- rowMeans(train_mat, na.rm=T)
i_means_vec <- colMeans(train_mat, na.rm=T)
ui_means_vec <- c(u_means_vec, i_means_vec)
mean_u_means <- mean(u_means_vec, na.rm=T)
mean_i_means <- mean(i_means_vec, na.rm=T)
u_sums_vec <- rowSums(train_mat, na.rm=T)
i_sums_vec <- colSums(train_mat, na.rm=T)
ui_sums_vec <- c(u_sums_vec, i_sums_vec)

### Set up solvable matrix version of Bayes / ridge regression by adding to the diagonal of A^T A
K <- 3
ATA <- unname(train_mat_bin)
diag(ATA) <- diag(ATA) + K
ATb <- unname(ui_sums_vec)
ATb <- ATb + c(rep(mean_u_means, length(u_means_vec)), rep(mean_i_means, length(i_means_vec))) * K

### Add intercept by adding a row and column to ATA which is total number of ratings (including the added Bayes ratings) 
###### concatenated on both sides with number of user (item) ratings (in this case it is diagonal of ATA matrix bc A is
###### all 1s and 0s, so don't need to worry about squared terms, i.e., 1^2 = 1)
### And add a value to ATb which is total sum of ratings (including the added Bayes ratings)
ATA <- rbind(cbind(length(which(!is.na(train_mat))) + length(diag(ATA))*K, t(diag(ATA))), cbind(diag(ATA), ATA))
ATb <- c(sum(u_sums_vec) + sum(c(rep(mean_u_means, length(u_means_vec)), rep(mean_i_means, length(i_means_vec))) * K), ATb)

### Solve for x
x <- solve(ATA, ATb)

ATA[1:5, 1:5]
ATb[1:5]
x[1:5]

```

<br>

###Model evaluation

Armed with our intercept and slope coefficients, i.e., the new bias values, we can compare the RMSE of the resulting prediction matrix (performed on the held-out Test data) to the RMSE of the prediction matrix obtained using standard biases (i.e., simple averages).  

```{r model_eval}

### Populate a prediction matrix using the regression coefficients
train_mat_pred <- matrix(NA, nrow = nrow(train_mat), ncol=ncol(train_mat))
for (i in 1:nrow(train_mat_pred)){
  train_mat_pred[i, ] <- unname(unlist(x[1] + x[i+1] + x[(length(u_means_vec)+2):length(x)]))
}
train_mat_pred[train_mat_pred<1] <- 1   #round anything below 1 or above 5
train_mat_pred[train_mat_pred>5] <- 5
train_mat_norm <- train_mat - train_mat_pred
train_mat_norm[is.na(train_mat_norm)] <- 0

### Prediction matrix using standard baseline
train_mat_pred_simp <- train_mat_norm_simp + baseline
train_mat_pred_simp[train_mat_pred_simp<1] <- 1
train_mat_pred_simp[train_mat_pred_simp>5] <- 5

### Calculate RMSE on Test data
rmse <- function(predicted, observed, rmna=FALSE){
  sqrt(mean((predicted - observed)^2, na.rm=rmna))
}

### Regression based
rmse_pred <- rmse(train_mat_pred, test_mat, rmna=T)
rmse_pred

### Simple baseline
rmse_pred_simp <- rmse(train_mat_pred_simp, test_mat, rmna=T)
rmse_pred_simp

```

As we can see, the regression baseline yields more accurate predictions than the standard baseline!  

<br>

###Matrix factorization

We can elaborate on both of our models by using Singular Value Decomposition on the residuals (i.e., observed rating minus baseline predicted rating). This effectively finds latent factors or "themes" in the values over and above the baseline. In other words, if Bruce rated both Inception and Blade Runner a half point higher than we predicted according to baseline, maybe it was not random but part of a pattern that Bruce likes science-fiction type movies. Matrix factorization techniques seek out these patterns by finding hidden correlations and accordingly reducing dimensionality, though the new themes/factors are not directly interpretable like the example.  

We will run SVD on both models and compare the RMSE:  

```{r mat_fac_svd}

### Perform SVD on regression baseline
svd_list_train <- svd(train_mat_norm)
k <- 10   #compress to 10 dimensions
u_comp <- svd_list_train$u[, 1:k]
d_comp <- svd_list_train$d[1:k]
v_comp <- svd_list_train$v[, 1:k]
train_mat_norm_comp <- u_comp %*% diag(d_comp) %*% t(v_comp)
dimnames(train_mat_norm_comp) <- dimnames(train_mat_norm)
train_mat_pred_svd <- train_mat_norm_comp + train_mat_pred
train_mat_pred_svd[train_mat_pred_svd<1] <- 1
train_mat_pred_svd[train_mat_pred_svd>5] <- 5

### RMSE on test data
rmse_pred_svd <- rmse(train_mat_pred_svd, test_mat, rmna=T)
rmse_pred_svd

### Perform SVD on standard baseline
svd_list_train_simp <- svd(train_mat_norm_simp)
u_comp <- svd_list_train$u[, 1:k]
d_comp <- svd_list_train$d[1:k]
v_comp <- svd_list_train$v[, 1:k]
train_mat_norm_simp_comp <- u_comp %*% diag(d_comp) %*% t(v_comp)
dimnames(train_mat_norm_simp_comp) <- dimnames(train_mat_norm_simp)
train_mat_pred_simp_svd <- train_mat_norm_simp_comp + train_mat_pred_simp
train_mat_pred_simp_svd[train_mat_pred_simp_svd<1] <- 1
train_mat_pred_simp_svd[train_mat_pred_simp_svd>5] <- 5

### RMSE on test data
rmse_pred_simp_svd <- rmse(train_mat_pred_simp_svd, test_mat, rmna=T)
rmse_pred_simp_svd

```

Both RMSEs are lower than before, with the regression baseline version outperforming the standard baseline version.  

####SGD instead of SVD

Another commonly used matrix factorization method is Stochastic Gradient Descent. Known as "Funk SVD" in this context (after Simon Funk from the Netflix project), it is actually an iterative algorithm designed to closely approximate SVD. It is known to handle missing values a bit better than regular SVD (for which we had to impute zeros for NAs in order to run).  

We can try SGD instead of SVD (on both models) and compare the RMSE:  

```{r mat_fac_sgd}

### Perform Funk SVD on regression baseline
train_mat_norm <- train_mat - train_mat_pred   #go back to keeping NA values
f_svd_list_train <- funkSVD(train_mat_norm, k = k)
train_mat_norm_comp <- f_svd_list_train$U %*% t(f_svd_list_train$V)
dimnames(train_mat_norm_comp) <- dimnames(train_mat_norm)
train_mat_pred_sgd <- train_mat_norm_comp + train_mat_pred
train_mat_pred_sgd[train_mat_pred_sgd<1] <- 1
train_mat_pred_sgd[train_mat_pred_sgd>5] <- 5

### RMSE on test data
rmse_test_sgd <- rmse(train_mat_pred_sgd, test_mat, rmna=T)
rmse_test_sgd

### Perform Funk SVD on standard baseline
train_mat_norm_simp <- train_mat - baseline   #go back to keeping NA values
f_svd_list_train_simp <- funkSVD(train_mat_norm_simp, k = k)
train_mat_norm_simp_comp <- f_svd_list_train_simp$U %*% t(f_svd_list_train_simp$V)
dimnames(train_mat_norm_simp_comp) <- dimnames(train_mat_norm_simp)
train_mat_pred_simp_sgd <- train_mat_norm_simp_comp + train_mat_pred_simp
train_mat_pred_simp_sgd[train_mat_pred_simp_sgd<1] <- 1
train_mat_pred_simp_sgd[train_mat_pred_simp_sgd>5] <- 5

### RMSE on test data
rmse_test_simp_sgd <- rmse(train_mat_pred_simp_sgd, test_mat, rmna=T)
rmse_test_simp_sgd

```

Once again, both RMSEs are lower than they were without using matrix factorization, with the regression baseline version outperforming the standard baseline version.  

<br>

###Summary

Using a dataset of movie ratings from MovieLens, we built several different collaborative filtering recommender models.  

First we normalized our data and came up with baselines for users and items. The standard way of doing this is to use simple averages, but instead we ran a regression. Due to computational expense we used a few linear algebra tricks to speed up the regression. Next we explored dimensionality reduction using two different forms of matrix factorization.  

We evaluated our various models by training them on a subset of the original data and then calculating the RMSE of predictions on the unseen test data. In doing so, we saw more accurate predictions with regression baselines compared to standard baselines, and with matrix factorization compared to no matrix factorization.  

Future work could explore the distributions of the biases. Linear regression did a good job of improving point estimates for users and items, but additionally examining the shapes of user and item distributions might prove beneficial.
